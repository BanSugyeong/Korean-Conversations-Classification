{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 맞춤법 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"j5ng/et5-typos-corrector\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"j5ng/et5-typos-corrector\")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 텍스트 항목에 대한 맞춤법 검사 및 교정\n",
    "for key, text in data.items():\n",
    "    input_text = \"맞춤법을 고쳐주세요: \" + text\n",
    "    input_encoding = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    input_ids = input_encoding.input_ids.to(device)\n",
    "    attention_mask = input_encoding.attention_mask.to(device)\n",
    "\n",
    "    output_encoding = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    output_text = tokenizer.decode(output_encoding[0], skip_special_tokens=True)\n",
    "    correctedspell_texts[key] = output_text\n",
    "    print(f'Corrected: {output_text}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# KoNLPy의 Okt 형태소 분석기 인스턴스 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 수행\n",
    "for path, content in correctedspell_texts.items():\n",
    "    # 텍스트의 형태소 분석\n",
    "    tokens = okt.morphs(content)\n",
    "    print(f'File: {path}\\nTokens: {tokens}\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
